{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVfDC-pjDwYK","executionInfo":{"status":"ok","timestamp":1672953363641,"user_tz":-60,"elapsed":977,"user":{"displayName":"Max Luo","userId":"08164895898074752725"}},"outputId":"a56422e2-87b2-4235-be48-46574e6f3680"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2021 NVIDIA Corporation\n","Built on Sun_Feb_14_21:12:58_PST_2021\n","Cuda compilation tools, release 11.2, V11.2.152\n","Build cuda_11.2.r11.2/compiler.29618528_0\n","Thu Jan  5 21:16:02 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    26W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvcc --version\n","!nvidia-smi"]},{"cell_type":"code","source":["%%writefile vectorAdd.cu\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <time.h>\n","#include <sys/time.h>\n","#include <math.h>\n","\n","#define DataType double\n","#define TPB 256\n","#define MAXTHREAD 1024\n","\n","__global__ void vecAdd(DataType *in1, DataType *in2, DataType *out, int len) {\n","  //@@ Insert code to implement vector addition here\n","\n","  int id = blockIdx.x*blockDim.x+threadIdx.x;\n"," \n","    if (id < len)\n","        out[id] = in1[id] + in2[id];\n","\n","}\n","\n","\n","//@@ Insert code to implement timer start\n","double timestart(){\n","  struct timeval t1;\n","  gettimeofday(&t1, NULL);\n","  return (double) (1000000.0*(t1.tv_sec) + t1.tv_usec)/1000000.0;\n","\n","}\n","\n","//@@ Insert code to implement timer stop\n","double timestop(double t1){\n","  struct timeval t2;\n","  gettimeofday(&t2, NULL);\n","  return (double) (1000000.0*(t2.tv_sec) + t2.tv_usec)/1000000.0 - t1;\n","}\n","\n","\n","int main(int argc, char **argv) {\n","  \n","  int inputLength;\n","  double timecost;\n","  double start;\n","  DataType *hostInput1;\n","  DataType *hostInput2;\n","  DataType *hostOutput;\n","  DataType *resultRef;\n","  DataType *deviceInput1;\n","  DataType *deviceInput2;\n","  DataType *deviceOutput;\n","\n","  //@@ Insert code below to read in inputLength from args\n","\n","  printf(\"testing\");\n","  \n","  inputLength = atoi (argv[1]);\n","  printf(\"The input length is %d\\n\", inputLength);\n","  \n","  //@@ Insert code below to allocate Host memory for input and output\n","\n","  hostInput1 = (DataType*) malloc(inputLength * sizeof(DataType));\n","  hostInput2 = (DataType*) malloc(inputLength * sizeof(DataType));\n","  hostOutput = (DataType*) malloc(inputLength * sizeof(DataType));\n","  resultRef = (DataType*) malloc(inputLength * sizeof(DataType));\n","  \n","  \n","  //@@ Insert code below to initialize hostInput1 and hostInput2 to random numbers, and create reference result in CPU\n","\n","  for (int i=0;i<inputLength;i++){\n","      hostInput1[i] = rand()/(DataType)RAND_MAX;\n","      hostInput2[i] = rand()/(DataType)RAND_MAX;\n","      resultRef[i] = hostInput1[i] + hostInput2[i]; \n","  }\n","\n","\n","  //@@ Insert code below to allocate GPU memory here\n","\n","\n","\n","  cudaMalloc(&deviceInput1, inputLength * sizeof(DataType));\n","  cudaMalloc(&deviceInput2, inputLength * sizeof(DataType));\n","  cudaMalloc(&deviceOutput, inputLength * sizeof(DataType));\n","  \n","\n","\n","\n","\n","  //@@ Insert code to below to Copy memory to the GPU here\n","\n","  start = timestart();\n","\n","  cudaMemcpy(deviceInput1, hostInput1, inputLength * sizeof(DataType), cudaMemcpyHostToDevice);\n","  cudaMemcpy(deviceInput2, hostInput2, inputLength * sizeof(DataType), cudaMemcpyHostToDevice);\n","  cudaMemcpy(deviceOutput, hostOutput, inputLength * sizeof(DataType), cudaMemcpyHostToDevice);\n","  cudaDeviceSynchronize();\n","  timecost = timestop(start);\n","  printf(\"Copying memory host to device cost %f seconds\\n\", timecost);\n","\n","\n","\n","  //@@ Initialize the 1D grid and block dimensions here\n","\n","  int gridnum = (inputLength+TPB-1)/TPB;\n","  int blocksize = TPB;\n","\n","\n","  //@@ Launch the GPU Kernel here\n","\n","  double start1 = timestart();\n","  vecAdd<<<gridnum, blocksize>>>(deviceInput1, deviceInput2, deviceOutput, inputLength);\n","  cudaDeviceSynchronize();\n","  double timecost1 = timestop(start1);\n","  printf(\"launch the kernel cost %f seconds\\n\", timecost1);\n","\n","\n","  //@@ Copy the GPU memory back to the CPU here\n","\n","  double start2 = timestart();\n","  cudaMemcpy(hostOutput, deviceOutput, inputLength * sizeof(DataType), cudaMemcpyDeviceToHost);\n","  double timecost2 = timestop(start2);\n","  printf(\"copy GPU memory to CPU cost %f seconds\\n\", timecost2);\n","\n","  //@@ Insert code below to compare the output with the reference\n","\n","  int notequal = 0;\n","  DataType diff = 1e-8;\n","  for(int i=0;i<inputLength;i++){\n","      if (abs(hostOutput[i] - resultRef[i]) > diff) {\n","          notequal = 1;\n","      }\n","  }\n","\n","  if (notequal == 1){\n","      printf(\"The result is different from refernce!\");\n","  }\n","  else if (notequal == 0){\n","      printf(\"The result aligns with the refernce!\");\n","  }\n","    \n","\n","\n","  //@@ Free the GPU memory here\n","  cudaFree(deviceInput1);\n","  cudaFree(deviceInput2);\n","  cudaFree(deviceOutput);\n","\n","  //@@ Free the CPU memory here\n","\n","  free(hostInput1);\n","  free(hostInput2);\n","  free(hostOutput);\n","  free(resultRef);\n","\n","  return 0;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AGh9lirYOYlo","executionInfo":{"status":"ok","timestamp":1672953596563,"user_tz":-60,"elapsed":199,"user":{"displayName":"Max Luo","userId":"08164895898074752725"}},"outputId":"38a75bcf-d772-4bf5-c679-98153ba99259"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting vectorAdd.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 vectorAdd.cu -o vectorAdd\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XA-mH7di1tIS","executionInfo":{"status":"ok","timestamp":1672953605538,"user_tz":-60,"elapsed":870,"user":{"displayName":"Max Luo","userId":"08164895898074752725"}},"outputId":"5225f530-29d9-48bf-e11f-9ec4efaa5517"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data  vectorAdd\tvectorAdd.cu\n"]}]},{"cell_type":"code","source":["!/usr/local/cuda-11/bin/nv-nsight-cu-cli ./vectorAdd 24680000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tk_rzR-F1vUF","executionInfo":{"status":"ok","timestamp":1672954198281,"user_tz":-60,"elapsed":3552,"user":{"displayName":"Max Luo","userId":"08164895898074752725"}},"outputId":"f958cf35-542b-40fd-afca-02c716d9fd86"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["testingThe input length is 24680000\n","==PROF== Connected to process 4149 (/content/vectorAdd)\n","Copying memory host to device cost 0.151735 seconds\n","==PROF== Profiling \"vecAdd\" - 1: 0%....50%....100% - 8 passes\n","launch the kernel cost 0.718206 seconds\n","copy GPU memory to CPU cost 0.182792 seconds\n","The result aligns with the refernce!==PROF== Disconnected from process 4149\n","[4149] vectorAdd@127.0.0.1\n","  vecAdd(double*, double*, double*, int), 2023-Jan-05 21:29:56, Context 1, Stream 7\n","    Section: GPU Speed Of Light\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    DRAM Frequency                                                           cycle/nsecond                           5.00\n","    SM Frequency                                                             cycle/usecond                         585.29\n","    Elapsed Cycles                                                                   cycle                      1,309,635\n","    Memory [%]                                                                           %                          90.29\n","    SOL DRAM                                                                             %                          90.29\n","    Duration                                                                       msecond                           2.24\n","    SOL L1/TEX Cache                                                                     %                          29.57\n","    SOL L2 Cache                                                                         %                          30.22\n","    SM Active Cycles                                                                 cycle                   1,305,516.75\n","    SM [%]                                                                               %                          23.56\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    OK    The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n","          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n","          Start by analyzing workloads in the Memory Workload Analysis section.                                         \n","\n","    Section: Launch Statistics\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    Block Size                                                                                                        256\n","    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n","    Grid Size                                                                                                      96,407\n","    Registers Per Thread                                                   register/thread                             16\n","    Shared Memory Configuration Size                                                 Kbyte                          32.77\n","    Driver Shared Memory Per Block                                              byte/block                              0\n","    Dynamic Shared Memory Per Block                                             byte/block                              0\n","    Static Shared Memory Per Block                                              byte/block                              0\n","    Threads                                                                         thread                     24,680,192\n","    Waves Per SM                                                                                                   602.54\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","\n","    Section: Occupancy\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    Block Limit SM                                                                   block                             16\n","    Block Limit Registers                                                            block                             16\n","    Block Limit Shared Mem                                                           block                             16\n","    Block Limit Warps                                                                block                              4\n","    Theoretical Active Warps per SM                                                   warp                             32\n","    Theoretical Occupancy                                                                %                            100\n","    Achieved Occupancy                                                                   %                          87.09\n","    Achieved Active Warps Per SM                                                      warp                          27.87\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","\n"]}]}]}