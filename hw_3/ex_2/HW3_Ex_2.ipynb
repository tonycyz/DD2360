{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1303,"status":"ok","timestamp":1672960047956,"user":{"displayName":"Max Luo","userId":"08164895898074752725"},"user_tz":-60},"id":"cPsRm7ieLese","outputId":"edd242df-a32c-4aa8-d1c2-28075e4e1030"},"outputs":[{"name":"stdout","output_type":"stream","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2021 NVIDIA Corporation\n","Built on Sun_Feb_14_21:12:58_PST_2021\n","Cuda compilation tools, release 11.2, V11.2.152\n","Build cuda_11.2.r11.2/compiler.29618528_0\n","Thu Jan  5 23:07:26 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   57C    P0    28W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvcc --version\n","!nvidia-smi"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1672960049209,"user":{"displayName":"Max Luo","userId":"08164895898074752725"},"user_tz":-60},"id":"4R32khIgLjak","outputId":"01fac43b-24c4-4b65-b1d0-2e2dc84c925a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting matrixMultiply.cu\n"]}],"source":["%%writefile matrixMultiply.cu\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <time.h>\n","#include <sys/time.h>\n","#include <math.h>\n","\n","#define DataType double\n","#define TPB 32\n","\n","// Compute C = A * B\n","\n","__global__ void gemm(DataType *A, DataType *B, DataType *C, int numARows,\n","                      int numAColumns, int numBRows, int numBColumns){\n","  //@@ Insert code to implement matrix multiplication here\n","  int col_id = blockIdx.x*blockDim.x+threadIdx.x;\n","  int row_id = blockIdx.y*blockDim.y+threadIdx.y;\n","\n","  if ((col_id<numBColumns)&&(row_id<numARows)){\n","    DataType addtmp=0;\n","    for (int i=0;i<numAColumns;i++){\n","      addtmp += A[numAColumns*row_id+i]*B[numBColumns*i+col_id];\n","    }\n","    C[numBColumns*row_id+col_id]=addtmp;\n","    \n","  }\n","}\n","\n","//@@ Insert code to implement timer start\n","double timestart(){\n","  struct timeval t1;\n","  gettimeofday(&t1, NULL);\n","  return (double) (1000000.0*(t1.tv_sec) + t1.tv_usec)/1000000.0;\n","\n","}\n","\n","//@@ Insert code to implement timer stop\n","double timestop(double t1){\n","  struct timeval t2;\n","  gettimeofday(&t2, NULL);\n","  return (double) (1000000.0*(t2.tv_sec) + t2.tv_usec)/1000000.0 - t1;\n","}\n","\n","\n","int main(int argc, char **argv) {\n","  \n","  DataType *hostA; // The A matrix\n","  DataType *hostB; // The B matrix\n","  DataType *hostC; // The output C matrix\n","  DataType *resultRef; // The reference result\n","  DataType *deviceA;\n","  DataType *deviceB;\n","  DataType *deviceC;\n","  int numARows;    // number of rows in the matrix A\n","  int numAColumns; // number of columns in the matrix A\n","  int numBRows;    // number of rows in the matrix B\n","  int numBColumns; // number of columns in the matrix B\n","  int numCRows;\n","  int numCColumns;\n","\n","  //@@ Insert code below to read in numARows, numAColumns, numBColumns from args\n","\n","  numARows = atoi(argv[1]);\n","  numAColumns = atoi(argv[2]);\n","  numBRows = atoi(argv[3]);\n","  numBColumns = atoi(argv[4]);\n","  numCRows = numARows;\n","  numCColumns = numBColumns;\n","\n","  printf(\"Input matrix dim (%d x %d) (%d x %d) (%d x %d)\\n\", numARows, numAColumns, numBRows, numBColumns, numCRows, numCColumns);\n","  \n","  //@@ Insert code below to allocate Host memory for input and output\n","  hostA = (DataType*) malloc(numARows * numAColumns * sizeof(DataType));\n","  hostB = (DataType*) malloc(numBRows * numBColumns * sizeof(DataType));\n","  hostC = (DataType*) malloc(numCRows * numCColumns * sizeof(DataType));\n","  resultRef = (DataType*) malloc(numCRows * numCColumns * sizeof(DataType));\n","  \n","  //@@ Insert code below to initialize hostA and hostB to random numbers, and create reference result in CPU\n","  for(int i=0;i<numARows;i++){\n","    for (int j=0;j<numAColumns;j++){\n","      //DataType randA=rand()/(DataType)RAND_MAX;\n","      hostA[i*numAColumns+j]=rand()/(DataType)RAND_MAX;\n","    }\n","  }\n","\n","    for(int i=0;i<numBRows;i++){\n","    for (int j=0;j<numBColumns;j++){\n","      //DataType randB=rand()/(DataType)RAND_MAX;\n","      hostB[i*numBColumns+j]=rand()/(DataType)RAND_MAX;\n","    }\n","  }\n","\n","  for(int i=0;i<numARows;i++) {\n","        for(int j=0;j<numBColumns;j++) {\n","          resultRef[i*numBColumns+j]=0;\n","          for(int k=0;k<numAColumns;k++) {\n","            resultRef[i*numBColumns+j]+=hostA[i*numAColumns+k]*hostB[k*numBColumns+j];\n","          }        \n","        }\n","  }\n","\n","  //@@ Insert code below to allocate GPU memory here\n","\n","  cudaMalloc(&deviceA, numARows * numAColumns * sizeof(DataType));\n","  cudaMalloc(&deviceB, numBRows * numBColumns * sizeof(DataType));\n","  cudaMalloc(&deviceC, numCRows * numCColumns * sizeof(DataType));\n","\n","\n","  //@@ Insert code to below to Copy memory to the GPU here\n","  double start1 = timestart();\n","  cudaMemcpy(deviceA, hostA, numARows * numAColumns * sizeof(DataType), cudaMemcpyHostToDevice);\n","  cudaMemcpy(deviceB, hostB, numBRows * numBColumns * sizeof(DataType), cudaMemcpyHostToDevice);\n","  double timecost1 = timestop(start1);\n","  printf(\"Copying memory cost %f seconds\\n\", timecost1);\n","  //@@ Initialize the grid and block dimensions here\n","\n","  int dimBlockx= TPB;\n","  int dimBlocky= TPB;\n","\n","  int dimGridx= (numCColumns+dimBlockx-1)/dimBlockx;\n","  int dimGridy= (numCRows+dimBlocky-1)/dimBlocky;\n","\n","  //@@ Launch the GPU Kernel here\n","  double start2=timestart();\n","  gemm<<<dim3(dimGridx,dimGridy,1),dim3(dimBlockx,dimBlocky,1)>>>(deviceA,deviceB,deviceC,numARows,numAColumns,numBRows,numBColumns);\n","  cudaDeviceSynchronize();\n","  double timecost2 = timestop(start2);\n","  printf(\"launch the kernel cost %f seconds\\n\", timecost2);\n","\n","\n","  //@@ Copy the GPU memory back to the CPU here\n","  double start3=timestart();\n","  cudaMemcpy(hostC, deviceC, numCRows * numCColumns * sizeof(DataType), cudaMemcpyDeviceToHost);\n","  double timecost3 = timestop(start3);\n","  printf(\"copy GPU memory to CPU cost %f seconds\\n\", timecost3);\n","\n","  //@@ Insert code below to compare the output with the reference\n","\n","  int notequal = 0;\n","  DataType diff = 1e-4;\n","  for(int i=0;i<numCRows;i++){\n","    for(int j=0;j<numCColumns;j++){\n","      if (fabs(hostC[numCColumns*i+j] - resultRef[numCColumns*i+j]) > diff) {\n","          notequal = 1;\n","          break;\n","    }\n","      }\n","  }\n","\n","  if (notequal == 1){\n","      printf(\"The result is different from refernce!\");\n","  }\n","  else if (notequal == 0){\n","      printf(\"The result aligns with the refernce!\");\n","  }\n","    \n","\n","\n","  //@@ Free the GPU memory here\n","  cudaFree(deviceA);\n","  cudaFree(deviceB);\n","  cudaFree(deviceC);\n","\n","  //@@ Free the CPU memory here\n","\n","  free(hostA);\n","  free(hostB);\n","  free(hostC);\n","  free(resultRef);\n","\n","  return 0;\n","}"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":909,"status":"ok","timestamp":1672960052645,"user":{"displayName":"Max Luo","userId":"08164895898074752725"},"user_tz":-60},"id":"SdTmC8RLL1Fx","outputId":"0ebd2494-dfb7-429a-839d-b07f99a84a4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["matrixMultiply\tmatrixMultiply.cu  sample_data\n"]}],"source":["!nvcc -arch=sm_75 matrixMultiply.cu -o matrixMultiply\n","!ls"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1259,"status":"ok","timestamp":1672960054825,"user":{"displayName":"Max Luo","userId":"08164895898074752725"},"user_tz":-60},"id":"nFbFtoA1eALE","outputId":"d4050c18-f3cf-4f0d-ef05-b0fb35700f62"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input matrix dim (122 x 128) (128 x 126) (122 x 126)\n","Copying memory cost 0.000100 seconds\n","launch the kernel cost 0.000146 seconds\n","copy GPU memory to CPU cost 0.000113 seconds\n","The result aligns with the refernce!"]}],"source":["!./matrixMultiply 122 128 128 126"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2806,"status":"ok","timestamp":1672960157027,"user":{"displayName":"Max Luo","userId":"08164895898074752725"},"user_tz":-60},"id":"71-ZmkuSMIDu","outputId":"ce1c71af-5bc9-4876-c60c-4b35feca273b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input matrix dim (100 x 234) (234 x 300) (100 x 300)\n","==PROF== Connected to process 8031 (/content/matrixMultiply)\n","Copying memory cost 0.000215 seconds\n","==PROF== Profiling \"gemm\" - 1: 0%....50%....100% - 8 passes\n","launch the kernel cost 0.643000 seconds\n","copy GPU memory to CPU cost 0.000193 seconds\n","The result aligns with the refernce!==PROF== Disconnected from process 8031\n","[8031] matrixMultiply@127.0.0.1\n","  gemm(double*, double*, double*, int, int, int, int), 2023-Jan-05 23:09:15, Context 1, Stream 7\n","    Section: GPU Speed Of Light\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    DRAM Frequency                                                           cycle/nsecond                           5.00\n","    SM Frequency                                                             cycle/usecond                         585.53\n","    Elapsed Cycles                                                                   cycle                        129,197\n","    Memory [%]                                                                           %                          18.25\n","    SOL DRAM                                                                             %                           1.66\n","    Duration                                                                       usecond                         220.64\n","    SOL L1/TEX Cache                                                                     %                          36.32\n","    SOL L2 Cache                                                                         %                           2.30\n","    SM Active Cycles                                                                 cycle                     100,278.05\n","    SM [%]                                                                               %                          72.45\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    WRN   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis report section to see     \n","          what the compute pipelines are spending their time doing. Also, consider whether any computation is           \n","          redundant and could be reduced or moved to look-up tables.                                                    \n","\n","    Section: Launch Statistics\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    Block Size                                                                                                      1,024\n","    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n","    Grid Size                                                                                                          40\n","    Registers Per Thread                                                   register/thread                             62\n","    Shared Memory Configuration Size                                                 Kbyte                          32.77\n","    Driver Shared Memory Per Block                                              byte/block                              0\n","    Dynamic Shared Memory Per Block                                             byte/block                              0\n","    Static Shared Memory Per Block                                              byte/block                              0\n","    Threads                                                                         thread                         40,960\n","    Waves Per SM                                                                                                        1\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the \n","          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   \n","          hardware busy.                                                                                                \n","\n","    Section: Occupancy\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","    Block Limit SM                                                                   block                             16\n","    Block Limit Registers                                                            block                              1\n","    Block Limit Shared Mem                                                           block                             16\n","    Block Limit Warps                                                                block                              1\n","    Theoretical Active Warps per SM                                                   warp                             32\n","    Theoretical Occupancy                                                                %                            100\n","    Achieved Occupancy                                                                   %                          93.40\n","    Achieved Active Warps Per SM                                                      warp                          29.89\n","    ---------------------------------------------------------------------- --------------- ------------------------------\n","\n"]}],"source":["!/usr/local/cuda-11/bin/nv-nsight-cu-cli ./matrixMultiply 100 234 234 300"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"yuzec","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"d9b6931df8223700d079595af72ee42686232424bf629c74cca75e856c83c900"}}},"nbformat":4,"nbformat_minor":0}
